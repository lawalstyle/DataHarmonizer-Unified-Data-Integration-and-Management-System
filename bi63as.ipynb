{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load and process CSV data\n",
    "def process_csv(file_path):\n",
    "    csv_data = pd.read_csv(file_path)\n",
    "    csv_data = csv_data.rename(columns=str.lower)  # Convert column names to lowercase\n",
    "    csv_data = csv_data.rename(columns={\n",
    "        'first name': 'firstname',\n",
    "        'second name': 'lastname',\n",
    "        'age (years)': 'age',\n",
    "        # Rename other columns if necessary\n",
    "    })\n",
    "    csv_data['age'] = csv_data['age'].astype(str)  # Standardize 'age' column to string type\n",
    "    return csv_data\n",
    "\n",
    "# Load and process JSON data\n",
    "def process_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    df = pd.json_normalize(json_data)\n",
    "    df = df.rename(columns=str.lower)  # Convert column names to lowercase\n",
    "    df['age'] = df['age'].astype(str)  # Standardize 'age' column to string type\n",
    "    return df\n",
    "\n",
    "# Load and process XML data\n",
    "def process_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    data = [user.attrib for user in root.findall('user')]\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.rename(columns=str.lower)  # Convert column names to lowercase\n",
    "    df['age'] = df['age'].astype(str)  # Standardize 'age' column to string type\n",
    "    return df\n",
    "\n",
    "# Define the file paths\n",
    "csv_path = \"C:\\\\Users\\\\mlawal\\\\Downloads\\\\Dissertation Papers\\\\cetm50_23_4_data (2)\\\\user_data_23_4.csv\"\n",
    "json_path = \"C:\\\\Users\\\\mlawal\\\\Downloads\\\\Dissertation Papers\\\\cetm50_23_4_data (2)\\\\user_data_23_4.json\"\n",
    "xml_path = \"C:\\\\Users\\\\mlawal\\\\Downloads\\\\Dissertation Papers\\\\cetm50_23_4_data (2)\\\\user_data_23_4.xml\"\n",
    "\n",
    "\n",
    "# Process the files\n",
    "csv_data = process_csv(csv_path)\n",
    "json_data = process_json(json_path)\n",
    "xml_data = process_xml(xml_path)\n",
    "\n",
    "# Merge the data on 'firstname' and 'lastname' columns\n",
    "merged_data = pd.merge(csv_data, json_data, on=['firstname', 'lastname'], how='outer', suffixes=('_csv', '_json'))\n",
    "merged_data = pd.merge(merged_data, xml_data, on=['firstname', 'lastname'], how='outer')\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940736c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'consolidated_age' column\n",
    "merged_data['consolidated_age'] = merged_data['age_csv']\n",
    "merged_data['consolidated_age'] = merged_data['consolidated_age'].combine_first(merged_data['age_json'])\n",
    "merged_data['consolidated_age'] = merged_data['consolidated_age'].combine_first(merged_data['age'])\n",
    "\n",
    "# Drop the original age columns\n",
    "merged_data.drop(columns=['age_csv', 'age_json', 'age'], inplace=True)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d87b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'consolidated_sex' column\n",
    "merged_data['consolidated_sex'] = merged_data['sex_x'].combine_first(merged_data['sex_y'])\n",
    "\n",
    "# Drop the original sex columns\n",
    "merged_data.drop(columns=['sex_x', 'sex_y'], inplace=True)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61efb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new 'consolidated_address_postcode' column\n",
    "merged_data['consolidated_address_postcode'] = merged_data['address_postcode_x'].combine_first(merged_data['address_postcode_y'])\n",
    "\n",
    "# Drop the original address_postcode columns\n",
    "merged_data.drop(columns=['address_postcode_x', 'address_postcode_y'], inplace=True)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all column names\n",
    "columns = list(merged_data.columns)\n",
    "\n",
    "# Remove 'consolidated_age' and 'consolidated_sex' from the list\n",
    "columns.remove('consolidated_age')\n",
    "columns.remove('consolidated_sex')\n",
    "\n",
    "# Insert 'consolidated_age' and 'consolidated_sex' right after 'lastname'\n",
    "lastname_index = columns.index('lastname')\n",
    "columns.insert(lastname_index + 1, 'consolidated_age')\n",
    "columns.insert(lastname_index + 2, 'consolidated_sex')\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "merged_data = merged_data[columns]\n",
    "\n",
    "# Check the updated DataFrame\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878d37c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns by removing the 'consolidated' prefix\n",
    "merged_data.rename(columns={\n",
    "    'consolidated_age': 'age',\n",
    "    'consolidated_sex': 'sex',\n",
    "    'consolidated_address_postcode': 'address_postcode'\n",
    "}, inplace=True)\n",
    "\n",
    "# Check the updated DataFrame\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245e0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "student_id = 'bi63as'\n",
    "\n",
    "config = {\n",
    "    'user': f'student_{student_id}',\n",
    "    'password': 'iE93F2@8EhM@1zhD&u9M@K',\n",
    "    'host': 'europa.ashley.work',\n",
    "    'database': f'student_{student_id}',\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Establish a database connection\n",
    "    cnx = mysql.connector.connect(**config)\n",
    "    print(\"Connection successfully established!\")\n",
    "\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if cnx.is_connected():\n",
    "        cnx.close()\n",
    "        print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfec600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame containing the merged data\n",
    "# merged_data = ...\n",
    "\n",
    "# Database configuration\n",
    "student_id = 'bi63as'\n",
    "config = {\n",
    "    'user': f'student_{student_id}',\n",
    "    'password': 'iE93F2@8EhM@1zhD&u9M@K',\n",
    "    'host': 'europa.ashley.work',\n",
    "    'database': f'student_{student_id}',\n",
    "}\n",
    "\n",
    "# Connect to the database\n",
    "cnx = mysql.connector.connect(**config)\n",
    "cursor = cnx.cursor()\n",
    "# SQL query to insert data\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO merged_records (\n",
    "    firstname, \n",
    "    lastname, \n",
    "    age,\n",
    "    sex,\n",
    "    vehicle_make, \n",
    "    vehicle_model, \n",
    "    vehicle_year, \n",
    "    vehicle_type, \n",
    "    iban, \n",
    "    credit_card_number, \n",
    "    credit_card_security_code, \n",
    "    credit_card_start_date, \n",
    "    credit_card_end_date, \n",
    "    address_main, \n",
    "    address_city, \n",
    "    debt, \n",
    "    debt_amount, \n",
    "    debt_time_period_years, \n",
    "    retired, \n",
    "    dependants, \n",
    "    marital_status, \n",
    "    salary, \n",
    "    pension, \n",
    "    company, \n",
    "    commute_distance, \n",
    "    address_postcode\n",
    ") VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "\n",
    "# Loop through the DataFrame and insert each row\n",
    "for i, row in merged_data.iterrows():\n",
    "    try:\n",
    "        # Convert all NaN values to None and create a tuple for insertion\n",
    "        data_tuple = tuple([None if pd.isna(value) else value for value in row])\n",
    "        cursor.execute(insert_query, data_tuple)\n",
    "    except Exception as e:\n",
    "        print(f\"Error inserting row {i}: {e}\")\n",
    "\n",
    "# Commit the changes to the database\n",
    "cnx.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b6ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
